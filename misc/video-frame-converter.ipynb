{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scripts contain experimental codes for video frame conversion. The codes are not well organized and may not work properly. Please use the codes in the main directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\"\"\"\n",
    "In this updated code, \n",
    "the transform_video function now takes in two arguments: \n",
    "\tvideo_path, which is the path to the input video,\n",
    "\tand output_dir, which is the directory to which the transformed video should be written. \n",
    "The transformed video is saved to a file using the tf.io.TFRecordWriter class, \n",
    "with the file name being the same as the input video. \n",
    "The transform_dataset function iterates over all subdirectories of the input dataset directory \n",
    "and applies the transform_video function to each video file. The transformed videos are written \n",
    "to separate directories with the same folder structure as the input dataset, \n",
    "which is specified by the output_root_dir argument.\n",
    "\"\"\"\n",
    "def transform_video(video_path, output_dir):\n",
    "    # Load the video using OpenCV\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    # Resize each frame to 224x224\n",
    "    video = [cv2.resize(frame, (224, 224)) for frame in video]\n",
    "\n",
    "    # Convert each frame to a tensor\n",
    "    video = [tf.constant(frame, dtype=tf.float32) for frame in video]\n",
    "\n",
    "    # Stack the list of tensors into a tensor of shape (num_frames, 3, 224, 224)\n",
    "    video = tf.stack(video, axis=0)\n",
    "\n",
    "    # Sample 30 random frames from the tensor\n",
    "    num_frames = video.shape[0]\n",
    "    frames_idx = random.sample(range(num_frames), 30)\n",
    "    video = tf.gather(video, frames_idx, axis=0)\n",
    "\n",
    "    # Write the transformed video to a file\n",
    "    output_path = os.path.join(output_dir, os.path.basename(video_path))\n",
    "    with tf.io.TFRecordWriter(output_path) as writer:\n",
    "        writer.write(tf.io.serialize_tensor(video))\n",
    "\n",
    "def transform_dataset(dataset_dir, output_dir):\n",
    "    for class_dir in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_dir)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        output_class_dir = os.path.join(output_dir, class_dir)\n",
    "        if not os.path.exists(output_class_dir):\n",
    "            os.makedirs(output_class_dir)\n",
    "\n",
    "        for video_file in os.listdir(class_path):\n",
    "            video_path = os.path.join(class_path, video_file)\n",
    "            transform_video(video_path, output_class_dir)\n",
    "\n",
    "# Example usage:\n",
    "# transform_dataset('path/to/dataset', 'path/to/transformed_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = '/Users/rudyhendrawan/miniforge3/datasets/small-Dasar-Gerakan-Tari-Bali-All-Women/test'\n",
    "dest_dir = '/Users/rudyhendrawan/miniforge3/datasets/small-Dasar-Gerakan-Tari-Bali-All-Women/test-224x224'\n",
    "transform_dataset(src_dir, dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, num_frames=30):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the total number of frames in the video\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Select num_frames random frames from the video\n",
    "    frame_indices = sorted(random.sample(range(total_frames), num_frames))\n",
    "\n",
    "    # Create a list to hold the frames\n",
    "    frames = []\n",
    "\n",
    "    # Loop through the selected frames\n",
    "    for i in frame_indices:\n",
    "        # Set the video to the ith frame\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "\n",
    "        # Read the frame\n",
    "        success, frame = video.read()\n",
    "\n",
    "        # Break the loop if we reached the end of the video\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Resize the frame to (224,224)\n",
    "        frame = cv2.resize(frame, (224, 224))\n",
    "\n",
    "        # Add the frame to the list of frames\n",
    "        frames.append(frame)\n",
    "\n",
    "    # Convert the list of frames to a tensor\n",
    "    frames = np.stack(frames, axis=0)\n",
    "\n",
    "    # Normalize the frames\n",
    "    frames = frames / 255.0\n",
    "\n",
    "    # Return the frames\n",
    "    return frames\n",
    "\n",
    "def video_to_tensor(video_path):\n",
    "    # Extract the frames from the video\n",
    "    frames = extract_frames(video_path)\n",
    "\n",
    "    # Convert the frames to a tensor\n",
    "    frames = tf.convert_to_tensor(frames, dtype=tf.float32)\n",
    "\n",
    "    # Return the frames\n",
    "    return frames\n",
    "\n",
    "def create_dataset(data_dir, batch_size=32, buffer_size=1024, repeat=None):\n",
    "    # Create a list to hold the file paths of all the videos\n",
    "    video_paths = []\n",
    "\n",
    "    # Loop through the subdirectories in data_dir\n",
    "    for class_dir in os.listdir(data_dir):\n",
    "        class_dir = os.path.join(data_dir, class_dir)\n",
    "\n",
    "        # Skip this iteration if class_dir is not a directory\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        # Loop through the videos in the class_dir\n",
    "        for video_file in os.listdir(class_dir):\n",
    "            video_path = os.path.join(class_dir, video_file)\n",
    "\n",
    "            # Skip this iteration if video_path is not a file\n",
    "            if not os.path.isfile(video_path):\n",
    "                continue\n",
    "\n",
    "            # Add the video_path to the list of video_paths\n",
    "            video_paths.append(video_path)\n",
    "\n",
    "    # Convert the list of video_paths to a tensor\n",
    "    video_paths = tf.constant(video_paths)\n",
    "\n",
    "    # Create a dataset from the video_paths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(video_paths)\n",
    "\t\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, here are some ways to optimize the code to increase its performance:\n",
    "\n",
    "1. Use the map function in the tf.data.Dataset API to preprocess the videos in parallel.\n",
    "\n",
    "2. Use the interleave function in the tf.data.Dataset API to interleave the video processing and loading operations, which can help to hide the loading time of one video behind the processing of another.\n",
    "\n",
    "3. Use the prefetch function in the tf.data.Dataset API to prefetch data to the GPU, allowing the GPU to perform computations while the CPU is loading data.\n",
    "\n",
    "4. Use a batch size that is large enough to make efficient use of GPU memory and compute resources, but not so large that it causes memory issues.\n",
    "\n",
    "5. Consider using TensorFlow's tf.data.TFRecordDataset to load the videos from disk, as it can be faster than loading from individual video files.\n",
    "\n",
    "6. Use tf.data.Dataset.cache to cache the preprocessed data in memory. This can be useful if the preprocessing takes a long time and you want to avoid repeating the same preprocessing for each epoch.\n",
    "\n",
    "7. If possible, consider converting the videos to a format that is more GPU-friendly, such as JPEG or PNG, and then use TensorFlow's tf.image functions to preprocess the data.\n",
    "\n",
    "Note: The specific optimizations that will be most effective will depend on the specifics of your dataset and hardware. Try experimenting with different combinations of these optimizations to find the best combination for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "def video_to_images(video_path, output_folder, num_frames=30, frame_size=(224,224)):\n",
    "    \"\"\"\n",
    "    Converts a video into a sequence of images and saves them in the output folder.\n",
    "    The output images will have the same folder structure as the input video.\n",
    "\n",
    "    Arguments:\n",
    "    video_path: str, path to the video file\n",
    "    output_folder: str, path to the folder where the image sequence will be saved\n",
    "    num_frames: int, number of frames to extract from the video\n",
    "    frame_size: tuple, size of the frames to be extracted\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Read the video\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Extract the video name and class from the video path\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    video_class = os.path.basename(os.path.dirname(video_path))\n",
    "\n",
    "    # Create the output folder\n",
    "    class_folder = os.path.join(output_folder, video_class)\n",
    "    if not os.path.exists(class_folder):\n",
    "        os.makedirs(class_folder)\n",
    "\n",
    "    # Create a transform to resize the frames\n",
    "    transform = transforms.Resize(frame_size)\n",
    "\n",
    "    # Initialize a counter for the number of frames\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        success, frame = video.read()\n",
    "\n",
    "        # Break the loop if the video has ended\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Apply the transform to resize the frame\n",
    "        frame = transform(frame)\n",
    "\n",
    "        # Convert the frame to a tensor\n",
    "        frame = torch.from_numpy(frame.numpy().transpose((2, 0, 1)))\n",
    "\n",
    "        # Save the frame as an image\n",
    "        image_path = os.path.join(class_folder, f\"{video_name}_{frame_count}.jpg\")\n",
    "        torch.save(frame, image_path)\n",
    "\n",
    "        # Increase the frame count\n",
    "        frame_count += 1\n",
    "\n",
    "        # Break the loop if the desired number of frames has been extracted\n",
    "        if frame_count >= num_frames:\n",
    "            break\n",
    "\n",
    "def convert_dataset(dataset_folder, output_folder, num_frames=30, frame_size=(224,224)):\n",
    "    \"\"\"\n",
    "    Converts the entire video dataset into a sequence of images and saves them in the output folder.\n",
    "    The output images will have the same folder structure as the dataset.\n",
    "\n",
    "    Arguments:\n",
    "    dataset_folder: str, path to the folder containing the dataset\n",
    "    output_folder: str, path to the folder where the image sequence will be saved\n",
    "    num_frames: int, number of frames to extract from each video\n",
    "    frame_size: tuple, size of the frames to be extracted\n",
    "    \n",
    "    Returns:\n",
    "\tNone\n",
    "\t\"\"\"\n",
    "\t# Loop over all the class folders in the dataset folder\n",
    "\tfor class_folder in os.listdir(dataset_folder):\n",
    "\t\tclass_folder = os.path.join(dataset_folder, class_folder)\n",
    "\t\t\n",
    "\t\t# Skip the class folder if it's not a directory\n",
    "\t\tif not os.path.isdir(class_folder):\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\t# Loop over all the video files in the class folder\n",
    "\t\tfor video_file in os.listdir(class_folder):\n",
    "\t\t\tvideo_path = os.path.join(class_folder, video_file)\n",
    "\t\t\t\n",
    "\t\t\t# Skip the file if it's not a video\n",
    "\t\t\tif not video_path.endswith((\".mp4\", \".avi\")):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\t# Convert the video into a sequence of images\n",
    "\t\t\tvideo_to_images(video_path, output_folder, num_frames, frame_size)\n",
    "            \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "In this code, the `video_to_images` function takes a video file and converts it into a sequence of images. The images are resized using the `transforms.Resize` transform, converted to a tensor using `torch.from_numpy`, and saved as JPEG images using `torch.save`.\n",
    "\n",
    "The `convert_dataset` function takes the entire video dataset and converts each video in the dataset into a sequence of images using the `video_to_images` function. The images are saved in the output folder, preserving the same folder structure as the original dataset.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = '/Users/rudyhendrawan/miniforge3/datasets/small-Dasar-Gerakan-Tari-Bali-All-Women/test/Agem_Kanan'\n",
    "dest_dir = '/Users/rudyhendrawan/miniforge3/datasets/small-Dasar-Gerakan-Tari-Bali-All-Women/test/Agem_Kanan-frames'\n",
    "\n",
    "# from video2images import Video2Images\n",
    "# Video2Images(video_filepath=src_dir,\n",
    "#              capture_rate=1,\n",
    "#              out_dir=dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for all videos to be completed. 0 videos\n",
      "This can take an hour or two depending on dataset size\n",
      "all done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "\"\"\"\n",
    "Given individual video files (mp4, webm) on disk, creates a folder for\n",
    "every video file and saves the video's RGB frames as jpeg files in that\n",
    "folder.\n",
    "It can be used to turn SomethingSomethingV2, which comes as \n",
    "many \".webm\" files, into an RGB folder for each \".webm\" file.\n",
    "Uses multithreading to extract frames faster.\n",
    "Modify the two filepaths at the bottom and then run this script.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def video_to_rgb(video_filename, out_dir, resize_shape):\n",
    "    file_template = 'frame_{0:012d}.jpg'\n",
    "    reader = cv2.VideoCapture(video_filename)\n",
    "    success, frame, = reader.read()  # read first frame\n",
    "\n",
    "    count = 0\n",
    "    while success:\n",
    "        out_filepath = os.path.join(out_dir, file_template.format(count))\n",
    "        frame = cv2.resize(frame, resize_shape)\n",
    "        cv2.imwrite(out_filepath, frame)\n",
    "        success, frame = reader.read()\n",
    "        count += 1\n",
    "\n",
    "def process_videofile(video_filename, video_path, rgb_out_path, file_extension: str ='.mp4'):\n",
    "    filepath = os.path.join(video_path, video_filename)\n",
    "    video_filename = video_filename.replace(file_extension, '')\n",
    "\n",
    "    out_dir = os.path.join(rgb_out_path, video_filename)\n",
    "    os.mkdir(out_dir)\n",
    "    video_to_rgb(filepath, out_dir, resize_shape=(224, 224))\n",
    "\n",
    "def thread_job(queue, video_path, rgb_out_path, file_extension='.webm'):\n",
    "    while not queue.empty():\n",
    "        video_filename = queue.get()\n",
    "        process_videofile(video_filename, video_path, rgb_out_path, file_extension=file_extension)\n",
    "        queue.task_done()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # the path to the folder which contains all video files (mp4, webm, or other)\n",
    "    video_path = src_dir\n",
    "    # the root output path where RGB frame folders should be created\n",
    "    rgb_out_path = dest_dir\n",
    "    # the file extension that the videos have\n",
    "    file_extension = '.webm'\n",
    "\n",
    "    video_filenames = os.listdir(video_path)\n",
    "    queue = Queue()\n",
    "    [queue.put(video_filename) for video_filename in video_filenames]\n",
    "\n",
    "    NUM_THREADS = 30\n",
    "    for i in range(NUM_THREADS):\n",
    "        worker = threading.Thread(target=thread_job, args=(queue, video_path, rgb_out_path, file_extension))\n",
    "        worker.start()\n",
    "\n",
    "    print('waiting for all videos to be completed.', queue.qsize(), 'videos')\n",
    "    print('This can take an hour or two depending on dataset size')\n",
    "    queue.join()\n",
    "    print('all done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2527e6f53b689e62fd6e328abffb3256ca5fab4c90ca1af42ac0843cf73f556"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
